{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import jieba\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "from os import listdir\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://new.qq.com/omn/20200730/20200730A0TCON00.html'      # 南方网\n",
    "south_paper = newspaper.build(url, language='zh')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.southcn.com/'          # 南方网"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寻找文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(key_word,dir = os.getcwd()):\n",
    "    file_paths = [os.path.join(dir, f) for f in listdir(dir) if os.path.isfile(os.path.join(dir, f)) and key_word in os.path.join(dir, f)][0]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘取新闻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_paper(url,filepath):\n",
    "    south_paper = newspaper.build(url,language='zh',memoize_articles = False)    # 构建新闻源\n",
    "    strings = \"{}{}{}{}{}{}\".format(\"品牌:\",south_paper.brand,\"描述:\",south_paper.description,\"共计:\",len(south_paper.articles))\n",
    "    news_title = []\n",
    "    news_text = []\n",
    "    news = south_paper.articles\n",
    "    for i in tqdm(range(len(news)),desc=strings):    # 以新闻链接的长度为循环次数\n",
    "        paper = news[i]\n",
    "        try :\n",
    "            paper.download()\n",
    "            paper.parse()\n",
    "            news_title.append(paper.title)     # 将新闻题目以列表形式逐一储存\n",
    "            news_text.append(paper.text)       # 将新闻正文以列表形式逐一储存\n",
    "        except:\n",
    "            news_title.append('NULL')          # 如果无法访问，以NULL替代\n",
    "            news_text.append('NULL')          \n",
    "            continue\n",
    "    # 建立数据表存储爬取的新闻信息\n",
    "    south_paper_data = pd.DataFrame({'title':news_title,'text':news_text})\n",
    "    south_paper_data = south_paper_data.drop_duplicates(subset=['text'], keep ='first')\n",
    "    south_paper_data.reset_index(drop=True)\n",
    "    south_paper_data.to_csv(filepath,mode=\"a\",header=False)\n",
    "    print(\"{}{}{}\".format(\"共计采集到<\",news.shape[0],\">篇新闻\"))\n",
    "    return south_paper_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 静态配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/停词库/stop_word_for_chinese.txt'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "corpus = find_file(\"南方网 3.csv\")\n",
    "stop_word_path = find_file(\"stop_word_for_chinese.txt\",\"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/停词库/\")\n",
    "stop_word_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本地csv去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_csv(file_path):\n",
    "    location_table = pd.read_csv(file_path)\n",
    "    start = location_table.shape[0]\n",
    "    location_table = location_table.drop_duplicates(subset=['text'], keep ='first')\n",
    "    location_table = location_table.reset_index(drop=True)\n",
    "    location_table.to_csv(file_path)\n",
    "    end = location_table.shape[0]\n",
    "    return start-end\n",
    "drop_duplicates_csv(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1217, 3)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始采集新闻数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "品牌:southcn描述:南方网/南方新闻网是经中共广东省委，广东省人民政府批准建设的新闻宣传网站。南方网/南方新闻网由广东省委宣传部主办主管并作为南方报业传媒集团之成员单位，获国务院新闻办公室批准从事登载新闻业务并被确定为全国重点新闻网站之一。南方网/南方新闻网作为华南地区最大型的新闻融合平台，是国内外网民认识、了解广东最权威、最快捷的途径。共计:1102:   0%|          | 0/1102 [00:00<?, ?it/s]Building prefix dict from /usr/local/lib/python3.7/site-packages/jieba/dict.txt ...\nLoading model from cache /var/folders/sl/q8x6_03132dfk7rktf00yh880000gn/T/jieba.cache\nLoading model cost 1.0406560897827148 seconds.\nPrefix dict has been built succesfully.\n品牌:southcn描述:南方网/南方新闻网是经中共广东省委，广东省人民政府批准建设的新闻宣传网站。南方网/南方新闻网由广东省委宣传部主办主管并作为南方报业传媒集团之成员单位，获国务院新闻办公室批准从事登载新闻业务并被确定为全国重点新闻网站之一。南方网/南方新闻网作为华南地区最大型的新闻融合平台，是国内外网民认识、了解广东最权威、最快捷的途径。共计:1102: 100%|██████████| 1102/1102 [10:45<00:00,  1.71it/s]\n"
    }
   ],
   "source": [
    "news = get_news_paper(url,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#临时删除文本元素\n",
    "def del_element(strings,symbles):\n",
    "    srcrep = {i:'' for i in symbles }\n",
    "    rep = dict((re.escape(k), v) for k, v in srcrep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], strings)\n",
    "\n",
    "#加载停用词\n",
    "stop_words = stop_words = open(stop_word_path,'r').read().split('\\n')+['\\n']\n",
    "\n",
    "#过滤停用词\n",
    "def filter_stop_word(paper,stop_words):\n",
    "    return np.array(list(filter(lambda x: x not in stop_words,jieba.cut(del_element(paper,'\\n')))))\n",
    "\n",
    "#读取本地新闻\n",
    "def read_txt(corpus):\n",
    "    return np.array([re.sub('\\n','',str(word)) for word in tqdm(pd.read_csv(corpus).text,desc='加载文章')])\n",
    "\n",
    "#只要中文\n",
    "def just_chinese(strings):\n",
    "    regStr = \".*?([\\u4E00-\\u9FA5]+).*?\"\n",
    "    expr = ''.join(re.findall(regStr, strings))\n",
    "    if expr:\n",
    "        return expr\n",
    "    return '\\n'\n",
    "\n",
    "#分词\n",
    "def split_word(original,temp_del=stop_words):\n",
    "    result = []\n",
    "    for paper in tqdm(original,desc='分词文章'):\n",
    "        chinese = just_chinese(paper)\n",
    "        temp_split_words = filter_stop_word(chinese,stop_words)\n",
    "        result.append(temp_split_words)\n",
    "    return np.array(result)\n",
    "\n",
    "# 排序字典\n",
    "def sort_dict(dict_items):\n",
    "    sorted_tuple = np.array(sorted(dict_items.items(), key=lambda x: x[0], reverse=True))\n",
    "    return dict(zip(sorted_tuple[:,0],sorted_tuple[:,1]))\n",
    "\n",
    "'''数据预处理函数'''\n",
    "def data_preprocessing(corpus):\n",
    "    # 读取原文\n",
    "    read_original = read_txt(corpus) \n",
    "    # 倒入文章并分词\n",
    "    init_paper = split_word(read_original,stop_words)\n",
    "    # 所有单词降维到一维\n",
    "    all_words = np.array([j for i in tqdm(init_paper,desc='词列表降维') for j in i])\n",
    "    # 单词去重\n",
    "    word_vector = np.unique(all_words)\n",
    "    # 测量共有词汇量\n",
    "    m = all_words.size\n",
    "    init_word_dict = {word:(all_words==word).dot(np.ones(m))/m for word in tqdm(word_vector,desc='构建频率词典')}\n",
    "    #构建排序字典和特征向量 \n",
    "    word_dict = sort_dict(init_word_dict)\n",
    "    word_vector = np.array(list(word_dict)) \n",
    "    return word_dict,word_vector,read_original,init_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "加载文章: 100%|██████████| 1217/1217 [00:00<00:00, 140840.11it/s]\n分词文章: 100%|██████████| 1217/1217 [00:50<00:00, 24.28it/s]\n词列表降维: 100%|██████████| 1217/1217 [00:00<00:00, 6845.88it/s]\n构建频率词典: 100%|██████████| 56256/56256 [06:01<00:00, 155.53it/s]\n"
    }
   ],
   "source": [
    "word_dict,word_vector,read_original,init_paper = data_preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-ITF 词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(paper_words,word_vector):\n",
    "    m = word_vector.size\n",
    "    init_TF = np.zeros(m)\n",
    "    for word in paper_words:\n",
    "        if word in word_vector:\n",
    "            index_ = np.argwhere(word_vector==word)[0][0]\n",
    "            init_TF[index_] += 1\n",
    "    return init_TF\n",
    "\n",
    "def IDF(paper_words_list,word_vector):\n",
    "    m = word_vector.size\n",
    "    init_IDF = np.zeros(m)\n",
    "    N = paper_words_list.shape\n",
    "    n = -1\n",
    "    for word in tqdm(word_vector,desc = 'IDF词汇'):\n",
    "        n += 1\n",
    "        for paper_arr in paper_words_list:\n",
    "            if word in paper_arr:\n",
    "                init_IDF[n] += 1\n",
    "    return np.log(N/(init_IDF+1))\n",
    "\n",
    "def TFIDF(paper_words_list,word_vector):\n",
    "    IDF_arr = IDF(init_paper,word_vector)\n",
    "    TF_arr = np.array([TF(paper,word_vector) for paper in tqdm(paper_words_list,desc = 'TF矩阵')])\n",
    "    return TF_arr*IDF_arr,IDF_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据源做TFIDF编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "IDF词汇: 100%|██████████| 56256/56256 [09:21<00:00, 100.20it/s]\nTF矩阵: 100%|██████████| 1217/1217 [07:57<00:00,  2.55it/s]\n"
    }
   ],
   "source": [
    "code_of_TFIDF,IDF = TFIDF(init_paper,word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征字典编辑器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建文章频率特征词向量\n",
    "def feature_dictionary_editor(words):\n",
    "    words_list = list(word_dict) #特征向量\n",
    "    feature_dict = dict(zip(words_list,np.zeros(len(words_list)))) # 特征字典\n",
    "    for word in words:\n",
    "        if word in words_list:\n",
    "            feature_dict[word]+=1\n",
    "    return np.array([frequency for word,frequency in feature_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1217, 56256)"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "code_of_TFIDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = init_paper[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "def try_index(n,arr):\n",
    "    try:\n",
    "        return (init_paper[n]==arr).all()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_index(arr):\n",
    "    for i in range(init_paper.shape[0]):\n",
    "        if try_index(i,arr):\n",
    "            return i\n",
    "find_index(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造倒排表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "倒排表当前排序的文章: 100%|██████████| 1217/1217 [03:55<00:00,  5.17it/s]\n"
    }
   ],
   "source": [
    "def inverted_index(paper,word_vector):\n",
    "    result = dict()\n",
    "    n = -1\n",
    "    for i in tqdm(paper,desc='倒排表当前排序的文章'):\n",
    "        n += 1\n",
    "        for j in i:\n",
    "            if j in word_vector:\n",
    "                if j in result:\n",
    "                    result[j] = result[j]+[n]\n",
    "                else:\n",
    "                    result.update({j:[n]})\n",
    "    return {i:list(set(result[i])) for i in result}\n",
    "\n",
    "Inverted_Index_List = inverted_index(init_paper,word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搜索引擎模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 搜索倒排表\n",
    "def search_inverted_index(strings,Inverted_Index_List):\n",
    "    words_for_search = []\n",
    "    split_word_for_search = [word for word in jieba.cut_for_search(strings) if word not in stop_words]\n",
    "    print(split_word_for_search)\n",
    "    for word in split_word_for_search:\n",
    "        if word in Inverted_Index_List:\n",
    "            print(\"\\n搜索单词:\",word,\"\\n文章序列:\",Inverted_Index_List[word])\n",
    "            words_for_search+=Inverted_Index_List[word]\n",
    "    return np.unique(np.array(words_for_search)),split_word_for_search\n",
    "\n",
    "#余弦相似度\n",
    "def cosine(s1,s2):\n",
    "    return s1.dot(s2)/(np.linalg.norm(s1) * np.linalg.norm(s2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征向量搜索入口函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def search(key,Inverted_Index_List):\n",
    "    search_paper_index,search_word = search_inverted_index(key,Inverted_Index_List)\n",
    "    search_result = []\n",
    "    search_prob = feature_dictionary_editor(search_word) #搜索内容的词向量\n",
    "    change_word_vector_from_words = init_paper[search_paper_index]\n",
    "    change_paper_from_words = read_original[search_paper_index]\n",
    "    for i in tqdm(range(len(change_paper_from_words)),desc='已经搜索数量'):\n",
    "        word_arr,paper = change_word_vector_from_words[i],change_paper_from_words[i]\n",
    "        paper_prob = feature_dictionary_editor(word_arr) #倒排表当前文章的词向量\n",
    "        cos = cosine(paper_prob,search_prob) #余弦相似度\n",
    "        parameter = cos \n",
    "        search_result.append([cos,paper])\n",
    "\n",
    "    search_result_arr = np.array(search_result)\n",
    "    result_table = pd.DataFrame({\"cos\":search_result_arr[:,0],\"newspaper\":search_result_arr[:,1]})\n",
    "    sort_table = result_table.sort_values([\"cos\"],ascending=False).reset_index(drop=True)\n",
    "    file_path = \"{}{}{}\".format(os.getcwd(),\"/\",key)\n",
    "    sort_table.to_csv(file_path)\n",
    "    return sort_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "已经搜索数量:   0%|          | 0/6 [00:00<?, ?it/s]['字节', '跳动']\n\n搜索单词: 字节 \n文章序列: [337, 1177, 1053, 1214, 1023]\n\n搜索单词: 跳动 \n文章序列: [609, 337, 1177, 1053, 1214, 1023]\n已经搜索数量: 100%|██████████| 6/6 [00:02<00:00,  2.99it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   cos  \\\n0  0.24723408821707438   \n1  0.09968895725584535   \n2  0.05369248441712195   \n3   0.0410132391597531   \n4  0.02945000320268802   \n5  0.01251173525528371   \n\n                                                                                             newspaper  \n0  8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了TikTok美国业务面临被CFIUS（美国外资投资委员会）强制要求出售的问题。8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了T...  \n1  分别都是哪些企业？中国有227家企业上榜最新的胡润全球高成长性企业榜，大湾区上榜者有33家。8月4日，胡润研究院发布2020年高成长性企业榜单，列出了全球成立于2000年之后、价值10亿美元以...  \n2  8月4日，胡润研究院发布了“2020年全球高成长性企业”榜单显示，全球上榜有586家企业，上榜企业总价值12.9万亿，企业平均年龄仅9岁。中国仍是全球高成长性企业诞生的热土，共227家企业入围...  \n3  详情·钟南山成为共和国勋章建议人选·台风“黑格比”将带来强降水·广州地铁八号线北延段今年内开通·支持深圳等升直辖市论文作者回应·大连疫情可能始于海产品加工车间·录取通知书不得投递至快件箱自提点...  \n4  7月26日，理想汽车向美国SEC更新招股书，宣布IPO股票公开发行规模为9500万股ADS，发行价区间为8美元至10美元。按照目前公布的发行价格，此次理想汽车IPO融资总额将达到12.54亿美...  \n5  自4月底公告股票停牌、宣布引战以来，泰禾股票先后拉出4个涨停板，股价从最低点4.06元一度追涨至7.7元，最大涨幅高达89.6%，呈现出一度爆棚的投资者信心。只听楼梯响，不见人下来。泰禾集团（...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cos</th>\n      <th>newspaper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.24723408821707438</td>\n      <td>8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了TikTok美国业务面临被CFIUS（美国外资投资委员会）强制要求出售的问题。8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了T...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.09968895725584535</td>\n      <td>分别都是哪些企业？中国有227家企业上榜最新的胡润全球高成长性企业榜，大湾区上榜者有33家。8月4日，胡润研究院发布2020年高成长性企业榜单，列出了全球成立于2000年之后、价值10亿美元以...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.05369248441712195</td>\n      <td>8月4日，胡润研究院发布了“2020年全球高成长性企业”榜单显示，全球上榜有586家企业，上榜企业总价值12.9万亿，企业平均年龄仅9岁。中国仍是全球高成长性企业诞生的热土，共227家企业入围...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0410132391597531</td>\n      <td>详情·钟南山成为共和国勋章建议人选·台风“黑格比”将带来强降水·广州地铁八号线北延段今年内开通·支持深圳等升直辖市论文作者回应·大连疫情可能始于海产品加工车间·录取通知书不得投递至快件箱自提点...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.02945000320268802</td>\n      <td>7月26日，理想汽车向美国SEC更新招股书，宣布IPO股票公开发行规模为9500万股ADS，发行价区间为8美元至10美元。按照目前公布的发行价格，此次理想汽车IPO融资总额将达到12.54亿美...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.01251173525528371</td>\n      <td>自4月底公告股票停牌、宣布引战以来，泰禾股票先后拉出4个涨停板，股价从最低点4.06元一度追涨至7.7元，最大涨幅高达89.6%，呈现出一度爆棚的投资者信心。只听楼梯响，不见人下来。泰禾集团（...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "TFsearch = search(\"字节跳动\",Inverted_Index_List)\n",
    "TFsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITITF 向量搜索入口函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "已经搜索数量: 100%|██████████| 11/11 [00:00<00:00, 392.25it/s]['字节', '跳动', '特朗普']\n\n搜索单词: 字节 \n文章序列: [337, 1177, 1053, 1214, 1023]\n\n搜索单词: 跳动 \n文章序列: [609, 337, 1177, 1053, 1214, 1023]\n\n搜索单词: 特朗普 \n文章序列: [836, 1190, 1068, 504, 1053, 767]\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     cos  \\\n0     0.3229172319362852   \n1    0.12397932268025351   \n2    0.08859123118372572   \n3    0.06336359525339914   \n4    0.03627028503894913   \n5   0.031872194315176294   \n6   0.028382747043877173   \n7    0.01871342292390454   \n8    0.01624032743746218   \n9   0.014056927810600181   \n10  0.012432016830607795   \n\n                                                                                              newspaper  \n0   8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了TikTok美国业务面临被CFIUS（美国外资投资委员会）强制要求出售的问题。8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了T...  \n1   分别都是哪些企业？中国有227家企业上榜最新的胡润全球高成长性企业榜，大湾区上榜者有33家。8月4日，胡润研究院发布2020年高成长性企业榜单，列出了全球成立于2000年之后、价值10亿美元以...  \n2   8月4日，胡润研究院发布了“2020年全球高成长性企业”榜单显示，全球上榜有586家企业，上榜企业总价值12.9万亿，企业平均年龄仅9岁。中国仍是全球高成长性企业诞生的热土，共227家企业入围...  \n3   详情·钟南山成为共和国勋章建议人选·台风“黑格比”将带来强降水·广州地铁八号线北延段今年内开通·支持深圳等升直辖市论文作者回应·大连疫情可能始于海产品加工车间·录取通知书不得投递至快件箱自提点...  \n4   7月26日，理想汽车向美国SEC更新招股书，宣布IPO股票公开发行规模为9500万股ADS，发行价区间为8美元至10美元。按照目前公布的发行价格，此次理想汽车IPO融资总额将达到12.54亿美...  \n5   “法轮功”是被中国政府依法取缔的邪教组织。这样一个反社会、反科学、反人类的邪教团体，却长期受到国外资助，混迹于香港街头，依托香港为跳板进行非法活动。但在香港国安法制定以后，部分“法轮功”分子告...  \n6   汪文斌在当日例行记者会上说，一段时间以来，美方在拿不出任何证据的情况下，泛化国家安全概念，滥用国家力量，无理打压特定的非美国企业，这违背市场经济原则，也违反世贸组织开放、透明、非歧视原则，是赤...  \n7   我觉得中美贸易摩擦问题的发生有着深刻复杂的国际背景，当然也是中美关系的一部分。2019年4月中旬，“百名法学家百场报告会”省商务厅专场暨南粤法治报告会第五十八讲拟定在广州举办。现任中国社会科学...  \n8   新冠肺炎疫情在全球大流行，让不少似是而非的言论借助汲取疫情教训流行开来。疫情究竟改变了什么？世界到底应汲取什么教训？疫情暴发后，污名化也流行开来，这在提示我们，似是而非的言论成为又一种病毒，损...  \n9   自4月底公告股票停牌、宣布引战以来，泰禾股票先后拉出4个涨停板，股价从最低点4.06元一度追涨至7.7元，最大涨幅高达89.6%，呈现出一度爆棚的投资者信心。只听楼梯响，不见人下来。泰禾集团（...  \n10  “美国在疫情和经济面前都惨败了。”美国知名专栏作家保罗·克鲁格曼在《纽约时报》撰文发出哀叹。作为全球第一大经济体，美国严重的经济衰退和失控的疫情形势，为全球彻底战胜疫情和实现经济复苏带来诸多变...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cos</th>\n      <th>newspaper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.3229172319362852</td>\n      <td>8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了TikTok美国业务面临被CFIUS（美国外资投资委员会）强制要求出售的问题。8月3日中午，字节跳动创始人张一鸣发送公司全员信，回应了T...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.12397932268025351</td>\n      <td>分别都是哪些企业？中国有227家企业上榜最新的胡润全球高成长性企业榜，大湾区上榜者有33家。8月4日，胡润研究院发布2020年高成长性企业榜单，列出了全球成立于2000年之后、价值10亿美元以...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.08859123118372572</td>\n      <td>8月4日，胡润研究院发布了“2020年全球高成长性企业”榜单显示，全球上榜有586家企业，上榜企业总价值12.9万亿，企业平均年龄仅9岁。中国仍是全球高成长性企业诞生的热土，共227家企业入围...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.06336359525339914</td>\n      <td>详情·钟南山成为共和国勋章建议人选·台风“黑格比”将带来强降水·广州地铁八号线北延段今年内开通·支持深圳等升直辖市论文作者回应·大连疫情可能始于海产品加工车间·录取通知书不得投递至快件箱自提点...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.03627028503894913</td>\n      <td>7月26日，理想汽车向美国SEC更新招股书，宣布IPO股票公开发行规模为9500万股ADS，发行价区间为8美元至10美元。按照目前公布的发行价格，此次理想汽车IPO融资总额将达到12.54亿美...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.031872194315176294</td>\n      <td>“法轮功”是被中国政府依法取缔的邪教组织。这样一个反社会、反科学、反人类的邪教团体，却长期受到国外资助，混迹于香港街头，依托香港为跳板进行非法活动。但在香港国安法制定以后，部分“法轮功”分子告...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.028382747043877173</td>\n      <td>汪文斌在当日例行记者会上说，一段时间以来，美方在拿不出任何证据的情况下，泛化国家安全概念，滥用国家力量，无理打压特定的非美国企业，这违背市场经济原则，也违反世贸组织开放、透明、非歧视原则，是赤...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.01871342292390454</td>\n      <td>我觉得中美贸易摩擦问题的发生有着深刻复杂的国际背景，当然也是中美关系的一部分。2019年4月中旬，“百名法学家百场报告会”省商务厅专场暨南粤法治报告会第五十八讲拟定在广州举办。现任中国社会科学...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.01624032743746218</td>\n      <td>新冠肺炎疫情在全球大流行，让不少似是而非的言论借助汲取疫情教训流行开来。疫情究竟改变了什么？世界到底应汲取什么教训？疫情暴发后，污名化也流行开来，这在提示我们，似是而非的言论成为又一种病毒，损...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.014056927810600181</td>\n      <td>自4月底公告股票停牌、宣布引战以来，泰禾股票先后拉出4个涨停板，股价从最低点4.06元一度追涨至7.7元，最大涨幅高达89.6%，呈现出一度爆棚的投资者信心。只听楼梯响，不见人下来。泰禾集团（...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.012432016830607795</td>\n      <td>“美国在疫情和经济面前都惨败了。”美国知名专栏作家保罗·克鲁格曼在《纽约时报》撰文发出哀叹。作为全球第一大经济体，美国严重的经济衰退和失控的疫情形势，为全球彻底战胜疫情和实现经济复苏带来诸多变...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "\n",
    "def search_TFITF(key,Inverted_Index_List):\n",
    "    search_paper_index,search_word = search_inverted_index(key,Inverted_Index_List)\n",
    "    search_result = [] \n",
    "    TFITF_search = TF(search_word,word_vector)*IDF #搜索内容的词向量\n",
    "    change_word_vector_from_words = init_paper[search_paper_index]\n",
    "    change_paper_from_words = read_original[search_paper_index]\n",
    "    for i in tqdm(range(len(change_paper_from_words)),desc='已经搜索数量'):\n",
    "        word_arr,paper = change_word_vector_from_words[i],change_paper_from_words[i]\n",
    "        TFIDF_ROW = code_of_TFIDF[find_index(word_arr)]#倒排表当前文章的词向量z s\n",
    "        cos = cosine(TFIDF_ROW,TFITF_search) #余弦相似度\n",
    "        parameter = cos \n",
    "        search_result.append([cos,paper])\n",
    "    search_result_arr = np.array(search_result)\n",
    "    result_table = pd.DataFrame({\"cos\":search_result_arr[:,0],\"newspaper\":search_result_arr[:,1]})\n",
    "    sort_table = result_table.sort_values([\"cos\"],ascending=False).reset_index(drop=True)\n",
    "    file_path = \"{}{}{}\".format(os.getcwd(),\"/\",key)\n",
    "    sort_table.to_csv(file_path)\n",
    "    return sort_table\n",
    "\n",
    "TFITFsearch  = search_TFITF(input(),Inverted_Index_List)\n",
    "TFITFsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.mat(np.arange(100).reshape(10,10))    \n",
    "b = np.arange(100)\n",
    "\n",
    "def checktype(mat):\n",
    "    try:\n",
    "        if isinstance(mat,(np.ndarray)):\n",
    "            if len(mat.shape)>=2:\n",
    "                return mat\n",
    "            else:\n",
    "                print('shape error')\n",
    "        else:\n",
    "            print('type error')\n",
    "    except Exception as error:\n",
    "        return error\n",
    "\n",
    "checktype(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(function,\n array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n        [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n        [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n        [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n        [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n        [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]))"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "data = {\"name\":\"test\",\"lable\":lambda data : data.dot(np.ones(10)),\"data\":np.arange(100).reshape(10,10)}\n",
    "e = isinstance(data,dict)\n",
    "type(data['lable']),(data[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 45., 145., 245., 345., 445., 545., 645., 745., 845., 945.])"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "def load(data):\n",
    "    if isinstance(data,dict):\n",
    "        if isinstance(data[\"data\"],np.ndarray):\n",
    "            return data['lable'](data[\"data\"])\n",
    "load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}