{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "品牌:southcn描述:南方网/南方新闻网是经中共广东省委，广东省人民政府批准建设的新闻宣传网站。南方网/南方新闻网由广东省委宣传部主办主管并作为南方报业传媒集团之成员单位，获国务院新闻办公室批准从事登载新闻业务并被确定为全国重点新闻网站之一。南方网/南方新闻网作为华南地区最大型的新闻融合平台，是国内外网民认识、了解广东最权威、最快捷的途径。共计:1097: 100%|██████████| 1097/1097 [06:43<00:00,  2.72it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b8371f07618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#新闻采集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mget_news_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b8371f07618>\u001b[0m in \u001b[0;36mget_news_paper\u001b[0;34m(url, filepath)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# 建立数据表存储爬取的新闻信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msouth_paper_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnews_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnews_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'新闻采集完成，采集共计'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msouth_paper_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'篇'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0msouth_paper_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'http://www.southcn.com/'          # 南方网\n",
    "stop_word_path = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/停词库/stop_word_for_chinese.txt\"\n",
    "corpus = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/南方网.csv\"\n",
    "stop_words = list(map(lambda x : x.replace(\"\\n\",'') ,open(stop_word_path,'r').readlines()))\n",
    "\n",
    "\n",
    "def get_news_paper(url,filepath):\n",
    "    south_paper = newspaper.build(url,language='zh',memoize_articles = False)    # 构建新闻源\n",
    "    strings = \"{}{}{}{}{}{}\".format(\"品牌:\",south_paper.brand,\"描述:\",south_paper.description,\"共计:\",len(south_paper.articles))\n",
    "    news_title = []\n",
    "    news_text = []\n",
    "    news = south_paper.articles\n",
    "    for i in tqdm(range(len(news)),desc=strings):    # 以新闻链接的长度为循环次数\n",
    "        paper = news[i]\n",
    "        try :\n",
    "            paper.download()\n",
    "            paper.parse()\n",
    "            news_title.append(paper.title)     # 将新闻题目以列表形式逐一储存\n",
    "            news_text.append(paper.text)       # 将新闻正文以列表形式逐一储存\n",
    "        except:\n",
    "            news_title.append('NULL')          # 如果无法访问，以NULL替代\n",
    "            news_text.append('NULL')          \n",
    "            continue\n",
    "    # 建立数据表存储爬取的新闻信息\n",
    "    south_paper_data = pd.DataFrame({'title':news_title,'text':news_text})\n",
    "    print('新闻采集完成，采集共计',len(south_paper_data),'篇')\n",
    "    south_paper_data.to_csv(filepath)\n",
    "\n",
    "#构造列表删除临时元素\n",
    "def del_element(strings,symbles=''):\n",
    "    srcrep = {i:'' for i in symbles }\n",
    "    rep = dict((re.escape(k), v) for k, v in srcrep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], strings)\n",
    "\n",
    "#过滤停用词\n",
    "def filter_stop_word(strings,stop_word):\n",
    "    return ''.join([char for char in strings if char not in stop_words])\n",
    "\n",
    "#读取本地存储文本\n",
    "def read_txt(corpus):\n",
    "    return np.array([str(word) for word in pd.read_csv(corpus).text])\n",
    "\n",
    "#分词\n",
    "def split_word(original,temp_del='\\n'):\n",
    "    result = []\n",
    "    for paper in tqdm(original,desc='已分词文章数量'):\n",
    "        temp_split_words = np.array(list(jieba.cut(del_element(filter_stop_word(paper,stop_words),temp_del))))\n",
    "        result.append(temp_split_words)\n",
    "    return np.array(result)\n",
    "\n",
    "#新闻采集\n",
    "get_news_paper(url,corpus)\n",
    "\n",
    "def data_preprocessing(corpus):\n",
    "    # 读取原文\n",
    "    read_original = read_txt(corpus) \n",
    "    # 导入文章并分词\n",
    "    init_paper = split_word(read_original,stop_words)\n",
    "    # 所有单词降维到一维\n",
    "    all_words = np.array([j for i in init_paper for j in i])\n",
    "    # 单词去重\n",
    "    word_vector = np.unique(all_words)\n",
    "    # 测量共有词汇量\n",
    "    m = all_words.size\n",
    "    word_dict = dict()\n",
    "    for word in tqdm(word_vector,desc='构建频率词典'):\n",
    "        prob = (all_words==word).dot(np.ones(m))/m\n",
    "        temp = {word:prob}\n",
    "        word_dict.update(temp)\n",
    "    return word_dict,word_vector,read_original,init_paper\n",
    "print(\"共计单词:\",len(word_dict))\n",
    "\n",
    "#构造倒排表\n",
    "def inverted_index(paper,word_vector):\n",
    "    result = dict()\n",
    "    n = -1\n",
    "    for i in tqdm(paper,desc='倒排表当前排序的文章'):\n",
    "        n += 1\n",
    "        for j in i:\n",
    "            if j in word_vector:\n",
    "                if j in result:\n",
    "                    result[j] = result[j]+[n]\n",
    "                else:\n",
    "                    result.update({j:[n]})\n",
    "    return {i:list(set(result[i])) for i in result}\n",
    "\n",
    "#倒排表运行\n",
    "Inverted_Index_List = inverted_index(init_paper,word_vector)\n",
    "\n",
    "# 搜索倒排表\n",
    "def search_inverted_index(strings,Inverted_Index_List):\n",
    "    words_for_search = []\n",
    "    split_word_for_search = [word for word in jieba.cut_for_search(strings) if word not in stop_words]\n",
    "    print(split_word_for_search)\n",
    "    for word in tqdm(split_word_for_search,desc='搜索倒排表'):\n",
    "        if word in Inverted_Index_List:\n",
    "            #print(\"\\n搜索单词:\",word,\"\\n文章序列:\",Inverted_Index_List[word])\n",
    "            words_for_search+=Inverted_Index_List[word]\n",
    "    return np.unique(np.array(words_for_search)),split_word_for_search\n",
    "\n",
    "# 构建文章频率特征词向量\n",
    "def feature_dictionary_editor(words):\n",
    "    words_list = list(word_dict) #特征向量\n",
    "    feature_dict = dict(zip(words_list,np.zeros(len(words_list)))) # 特征字典\n",
    "    for word in words:\n",
    "        if word in words_list:\n",
    "            feature_dict[word]+=1\n",
    "    return np.array([frequency for word,frequency in feature_dict.items()])\n",
    "\n",
    "#余弦相似度\n",
    "def cosine(s1,s2):\n",
    "    return s1.dot(s2)/(np.linalg.norm(s1) * np.linalg.norm(s2))\n",
    "\n",
    "#搜索入口函数 \n",
    "def search(key,Inverted_Index_List):\n",
    "    search_paper_index,search_word = search_inverted_index(key,Inverted_Index_List) #\b倒排表删选\n",
    "    search_result = dict() \n",
    "    search_prob = feature_dictionary_editor(search_word) #搜索内容的词向量\n",
    "    change_word_vector_from_words = init_paper[search_paper_index]\n",
    "    change_paper_from_words = read_original[search_paper_index]\n",
    "    for i in tqdm(range(len(change_paper_from_words)),desc='已经搜索数量'):\n",
    "        word_arr,paper = change_word_vector_from_words[i],change_paper_from_words[i]\n",
    "        paper_prob = feature_dictionary_editor(word_arr) #倒排表当前文章的词向量\n",
    "        cos = cosine(paper_prob,search_prob) #余弦相似度\n",
    "        parameter = cos \n",
    "        search_result.update({parameter:paper}) #更新搜索结果\n",
    "    search_result_arr = np.array(sorted(search_result.items(), key=lambda x: x[0], reverse=True)) #对结果按照余弦值排序  \n",
    "    table = pd.DataFrame({'cos':search_result_arr[:,0],'newspaper':search_result_arr[:,0]})\n",
    "    return table #dict(zip(search_result_arr[:,0],search_result_arr[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-87a6a40a417d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"世界广东\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInverted_Index_List\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "search(\"世界广东\",Inverted_Index_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}