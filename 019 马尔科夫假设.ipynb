{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import jieba\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "from os import listdir\n",
    "import torch\n",
    "from IPython.display import display, Image\n",
    "from itertools import permutations\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "加载文章: 100%|██████████| 1217/1217 [00:00<00:00, 123025.91it/s]\n分词文章: 100%|██████████| 1217/1217 [00:19<00:00, 62.81it/s]\n词列表降维: 100%|██████████| 1217/1217 [00:00<00:00, 3308.64it/s]\n文章列表降维补码: 100%|██████████| 1217/1217 [00:00<00:00, 3094.20it/s]\n词列表转<tensor>编码: 100%|██████████| 1217/1217 [00:01<00:00, 871.37it/s]\n"
    }
   ],
   "source": [
    "def find_file(key_word,dir = os.getcwd()):\n",
    "    file_paths = [os.path.join(dir, f) for f in listdir(dir) if os.path.isfile(os.path.join(dir, f)) and key_word in os.path.join(dir, f)][0]\n",
    "    return file_paths\n",
    "\n",
    "# 加载文本\n",
    "corpus = find_file(\"南方网 3.csv\")\n",
    "stop_word_path = find_file(\"chinese_symbols.txt\",\"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/停词库/\")\n",
    "stop_word_path\n",
    "\n",
    "#临时删除文本元素\n",
    "def del_element(strings,symbles,Replace_the_symbol=\" @REPLACETHESYMBOL@ \"):\n",
    "    srcrep = {i:Replace_the_symbol for i in symbles }\n",
    "    rep = dict((re.escape(k), v) for k, v in srcrep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], strings)\n",
    "\n",
    "def replace_number(strings,NUMBERTOKEN = ' @NUMBERTOKEN@ '):\n",
    "    return re.sub(r'\\d+',NUMBERTOKEN,strings)\n",
    "\n",
    "#加载停用词\n",
    "stop_words = stop_words = ''.join(open(stop_word_path,'r').read().split('\\n')+['\\n'])\n",
    "\n",
    "#过滤停用词\n",
    "def filter_stop_word(paper,stop_words):\n",
    "    return np.array(list(filter(lambda x: x not in stop_words,jieba.cut(del_element(paper,'\\n')))))\n",
    "\n",
    "#读取本地新闻\n",
    "def read_txt(corpus):\n",
    "    return np.array([re.sub('\\n','',str(word)) for word in tqdm(pd.read_csv(corpus).text,desc='加载文章')])\n",
    "\n",
    "#只要中文\n",
    "def just_chinese(strings):\n",
    "    regStr = \".*?([\\u4E00-\\u9FA5]+).*?\"\n",
    "    expr = ''.join(re.findall(regStr, strings))\n",
    "    if expr:\n",
    "        return expr\n",
    "    return '\\n'\n",
    "\n",
    "#分词删除\n",
    "def split_word(original,temp_del=stop_words,just_chinese=False):\n",
    "    result = []\n",
    "    for paper in tqdm(original,desc='分词文章'):\n",
    "        if just_chinese:\n",
    "            paper_ = just_chinese(paper)\n",
    "        paper_1 = del_element(paper,stop_words)\n",
    "        paper_ = replace_number(paper_1)\n",
    "        temp_split_words = filter_stop_word(paper_,stop_words)\n",
    "        result.append(temp_split_words)\n",
    "    return np.array(result)\n",
    "\n",
    "# 排序字典\n",
    "def sort_dict(dict_items):\n",
    "    sorted_tuple = np.array(sorted(dict_items.items(), key=lambda x: x[0], reverse=True))\n",
    "    return dict(zip(sorted_tuple[:,0],sorted_tuple[:,1]))\n",
    "\n",
    "# 单词编码器\n",
    "def word_encoding(all_words):\n",
    "    unique,index_,counter= np.unique(all_words,return_counts=True,return_index=True)\n",
    "    encode_dict = dict(zip(unique,index_))\n",
    "    decode_dict = dict(zip(index_,unique))\n",
    "    prob_of_word_dict = dict(zip(unique,counter/unique.size))\n",
    "    prob_of_word_dict_to_log = dict(zip(unique,-np.log(counter/all_words.size)))\n",
    "    return decode_dict,encode_dict,prob_of_word_dict,prob_of_word_dict_to_log\n",
    "\n",
    "# 编码向量\n",
    "def encode_papers(split_paper,encode_dict):\n",
    "    # 所有单词降维到一维\n",
    "    return [torch.tensor([encode_dict[word] for word in paper]) for paper in tqdm(split_paper,desc='词列表转<tensor>编码')]\n",
    "\n",
    "def to_numpy(data):\n",
    "    if isinstance(data,np.ndarray):\n",
    "        return data\n",
    "    else:\n",
    "        return np.array(data)\n",
    "\n",
    "def INIT_Ngram(word_list,n):\n",
    "    m = word_list.size\n",
    "    end = np.array([np.array([word_list[j] for j in range(i) ][-n:]) for i in tqdm(range(m+1),desc='Ngram初始化')][1:])\n",
    "    return list(zip(word_list,end))\n",
    "\n",
    "'''数据预处理函数'''\n",
    "def data_preprocessing_to_tensor(corpus,stop_words=stop_words,just_chinese=False):\n",
    "    # 读取原文\n",
    "    read_original = read_txt(corpus) \n",
    "    # 倒入文章并分词\n",
    "    split_paper = split_word(read_original,stop_words,just_chinese=just_chinese)\n",
    "    # 所有单词降维到一维\n",
    "    all_words = np.array([j for i in tqdm(split_paper,desc='词列表降维') for j in i])\n",
    "    # 所有文章合并到一行并且补足符号\n",
    "    one_row = [word for paper in tqdm(split_paper,desc=\"文章列表降维补码\") for word in np.append(paper,'NEWLINE')]+['NEWLINE']\n",
    "    one_row.insert(0,'NEWLINE')\n",
    "    papers_to_one_dim = np.array(one_row)\n",
    "    # 单词编码\n",
    "    decode_dict,encode_dict,prob_of_word_dict,prob_of_word_dict_to_log = word_encoding(all_words)\n",
    "    # 转tensor\n",
    "    code_paper = encode_papers(split_paper,encode_dict)\n",
    "    # 编码向量\n",
    "    code_tensor = torch.unique(torch.tensor(list(decode_dict)))\n",
    "\n",
    "\n",
    "    return decode_dict,encode_dict,prob_of_word_dict,prob_of_word_dict_to_log,all_words,papers_to_one_dim,read_original,split_paper,code_tensor,code_paper\n",
    "\n",
    "\n",
    "'''TF-IDF torch'''\n",
    "def TF_Tensor(tensor_paper,code_tensor):\n",
    "    unique,counts = torch.unique(tensor_paper,return_counts=True)\n",
    "    init_TF = torch.zeros(code_tensor.size())\n",
    "    for e,c in zip(unique,counts):\n",
    "        if e in code_tensor:\n",
    "            index_ = np.where(code_tensor == e)[0][0]\n",
    "            init_TF[index_] = c\n",
    "    return init_TF\n",
    "\n",
    "def IDF_Tensor(tensor_papers,code_tensor):\n",
    "    N = len(tensor_papers)\n",
    "    NW = torch.zeros(code_tensor.size())\n",
    "    step = -1\n",
    "    for word_code in tqdm(code_tensor,desc=\"IDF词汇\"):\n",
    "        step += 1\n",
    "        counter = sum((word_code in paper_code for paper_code in tensor_papers))\n",
    "        NW[step] = counter\n",
    "    return torch.log(N/(NW+1))\n",
    "\n",
    "def TFIDFTensor(tensor_papers,code_tensor):\n",
    "    IDF = IDF_Tensor(tensor_papers,code_tensor)\n",
    "    m = len(tensor_papers)\n",
    "    n = int(code_tensor.size()[0])\n",
    "    DF = torch.zeros(m,n)\n",
    "    step = -1\n",
    "    for paper in tqdm(tensor_papers,desc=\"TF矩阵\"):\n",
    "        step += 1\n",
    "        DF[step] = TF_Tensor(paper,code_tensor)\n",
    "    return DF*IDF\n",
    "\n",
    "#静态配置\n",
    "decode_dict,encode_dict,prob_of_word_dict,prob_of_word_dict_to_log,all_words,papers_to_one_dim,read_original,split_paper,code_tensor,code_paper = data_preprocessing_to_tensor(corpus)\n",
    "\n",
    "# 初始化Ngram (耗时极长，且需要大量内存)\n",
    "#init_Ngram = INIT_Ngram(papers_to_one_dim,2)\n",
    "# torch训练TFIDF\n",
    "#TFIDF = TFIDFTensor(code_paper,code_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([(array([], dtype='<U3'), '特朗普'),\n  (array(['特朗普'], dtype='<U3'), '当局'),\n  (array(['特朗普', '当局'], dtype='<U3'), '似乎'),\n  (array(['特朗普', '当局', '似乎'], dtype='<U3'), '有点'),\n  (array(['当局', '似乎', '有点'], dtype='<U2'), '着急'),\n  (array(['似乎', '有点', '着急'], dtype='<U2'), '，'),\n  (array(['有点', '着急', '，'], dtype='<U2'), '这是'),\n  (array(['着急', '，', '这是'], dtype='<U2'), '一个'),\n  (array(['，', '这是', '一个'], dtype='<U2'), '好'),\n  (array(['这是', '一个', '好'], dtype='<U2'), '事情')],\n array(['特朗普', '当局', '似乎'], dtype='<U3'))"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def token(paper):\n",
    "    return np.array(list(jieba.cut(paper)))\n",
    "\n",
    "def Ngram(word_list,n):\n",
    "    sentence = to_numpy(word_list)\n",
    "    m = sentence.size\n",
    "    end = np.array([np.array([word_list[j] for j in range(i) ][-n:]) for i in range(m+1)][1:])\n",
    "    return [(e[:-1],e[-1]) for e in end]\n",
    "\n",
    "raw_text = token('特朗普当局似乎有点着急，这是一个好事情')\n",
    "Ngram(raw_text[:10],4),raw_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['特朗普', '政府', '上台'],\n       ['特朗普', '当局', '不断'],\n       ['特朗普', '的', '疯狂'],\n       ['特朗普', 'REPLACETHESYMBOL', '并'],\n       ['特朗普', '进行', '对话'],\n       ['特朗普', '政府', '全力'],\n       ['特朗普', 'NUMBERTOKEN', '日称']], dtype='<U16')"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "def following_words(word,n=2,word_docker=split_paper):\n",
    "    if word in list(stop_words):\n",
    "        word = \"REPLACETHESYMBOL\"\n",
    "    start = -1\n",
    "    result = []\n",
    "    for paper in word_docker:\n",
    "        start += 1\n",
    "        if word in paper:\n",
    "            ends = np.ravel(np.argwhere(paper==word))\n",
    "            for end in ends:\n",
    "                if end+n < paper.size:\n",
    "                    result.append(word_docker[start][end:end+n])\n",
    "                else:\n",
    "                    start_arr = word_docker[start][end:end+1]\n",
    "                    end_arr = [word]*(n-1)\n",
    "                    result.append(np.append(start_arr,end_arr))\n",
    "    return np.array(result)\n",
    "following_words(\"特朗普\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.9459101490553135"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "def join_prob(iterable,n,word_docker=split_paper):\n",
    "    start = iterable[0]\n",
    "    m = len(iterable)\n",
    "    if n > m:\n",
    "        n = m\n",
    "    check = (following_words(start,n,split_paper)==iterable[:n])\n",
    "    check = check.all(axis=1)\n",
    "    counter = check.sum()\n",
    "    if counter==0:\n",
    "        return 10**(-8)\n",
    "    return -np.log(counter/check.size)\n",
    "\n",
    "\n",
    "def check_prob(word,code_dict=prob_of_word_dict_to_log):\n",
    "    try:\n",
    "        return code_dict[word]\n",
    "    except Exception:\n",
    "        return 10**(-8)\n",
    "\n",
    "join_prob(('特朗普', '进行', '对话'),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "raw_text = token('特朗普当局似乎有点着急，这是一个好事情')\n",
    "prems = tuple(permutations(raw_text[:3]))\n",
    "temp = dict()\n",
    "for prem in prems:\n",
    "    start = prem[-1]\n",
    "    end = prem[:-1]\n",
    "    print(start,end)\n",
    "    PB = check_prob(start)\n",
    "    PAB = join_prob(end,len(end))\n",
    "    temp.update({PAB*PB:prem})\n",
    "temp = sorted(temp.items(), key=lambda x: x[-1], reverse=True)[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14.860931353357424, ['特朗普', '当局', '不断'])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "def check_path(start,end):\n",
    "\n",
    "    init_prems = tuple(permutations(start))\n",
    "\n",
    "    PB = check_prob(end)\n",
    "    if '0' in start:\n",
    "        return PB\n",
    "    temp = dict()\n",
    "    for prem in init_prems:\n",
    "        PAB = join_prob(prem,len(prem))\n",
    "        temp_list = list(prem)+[end]\n",
    "        temp.update({PAB*PB:temp_list})\n",
    "    temp = sorted(temp.items(), key=lambda x: x[-1], reverse=True)[0]\n",
    "    return temp\n",
    "\n",
    "raw_text = token('特朗普当局似乎有点着急，这是一个好事情')\n",
    "check_path(('特朗普', '当局', ),'不断')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{14.93042729740752: ['特朗普', '当局', '似乎', '有点', '着急']}"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def NGramLanguageModel(raw_text,n):\n",
    "    expr ,arr ,prob,result = 0,np.zeros(n),1,[]\n",
    "    \n",
    "    for gram in Ngram(raw_text,n):\n",
    "        end = gram[-1]\n",
    "        start = gram[0]\n",
    "        if start.size == 0:\n",
    "            prob += check_prob(end)\n",
    "            arr = ['']\n",
    "        else:\n",
    "            arr = np.append(start,end)\n",
    "            prob += join_prob(arr,arr.size)\n",
    "            arr = start\n",
    "        result.append(end)\n",
    " \n",
    "    return {prob:result}\n",
    "   \n",
    "raw_text = token('特朗普当局似乎有点着急')\n",
    "\n",
    "NGramLanguageModel(raw_text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TF-IDF torch'''\n",
    "def TF_Tensor(tensor_paper,code_tensor):\n",
    "    unique,counts = torch.unique(tensor_paper,return_counts=True)\n",
    "    init_TF = torch.zeros(code_tensor.size())\n",
    "    for e,c in zip(unique,counts):\n",
    "        if e in code_tensor:\n",
    "            index_ = np.where(code_tensor == e)[0][0]\n",
    "            init_TF[index_] = c\n",
    "    return init_TF\n",
    "\n",
    "def IDF_Tensor(tensor_papers,code_tensor):\n",
    "    N = len(tensor_papers)\n",
    "    NW = torch.zeros(code_tensor.size())\n",
    "    step = -1\n",
    "    for word_code in tqdm(code_tensor,desc=\"IDF词汇\"):\n",
    "        step += 1\n",
    "        counter = sum((word_code in paper_code for paper_code in tensor_papers))\n",
    "        NW[step] = counter\n",
    "    return torch.log(N/(NW+1))\n",
    "\n",
    "def TFIDFTensor(tensor_papers,code_tensor):\n",
    "    IDF = IDF_Tensor(tensor_papers,code_tensor)\n",
    "    m = len(tensor_papers)\n",
    "    n = int(code_tensor.size()[0])\n",
    "    DF = torch.zeros(m,n)\n",
    "    step = -1\n",
    "    for paper in tqdm(tensor_papers,desc=\"TF矩阵\"):\n",
    "        step += 1\n",
    "        DF[step] = TF_Tensor(paper,code_tensor)\n",
    "    return DF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " #TFIDF = TFIDFTensor(code_paper,code_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/B1F3D0BC-005D-4593-8D9F-E6016CB0AE61.png'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-742d59bd9c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/B1F3D0BC-005D-4593-8D9F-E6016CB0AE61.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0;32m-> 1204\u001b[0;31m                 metadata=metadata)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/B1F3D0BC-005D-4593-8D9F-E6016CB0AE61.png'"
     ]
    }
   ],
   "source": [
    "display( Image( filename = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/B1F3D0BC-005D-4593-8D9F-E6016CB0AE61.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: '/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/2537974B-8D80-4339-8774-C1EC4023BEF3.png'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             raise FileNotFoundError(\n\u001b[0;32m-> 1275\u001b[0;31m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[0m\u001b[1;32m   1276\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/2537974B-8D80-4339-8774-C1EC4023BEF3.png'"
     ]
    }
   ],
   "source": [
    "display(Image(\"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP训练营笔记/图片/2537974B-8D80-4339-8774-C1EC4023BEF3.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['特朗普', '当局', '似乎', '有点', '着急', '，', '这是', '一个', '好', '事情'],\n      dtype='<U3')"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = token('特朗普当局似乎有点着急，这是一个好事情')\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'find_code' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8d48fd7e9ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprob_of_word_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'find_code' is not defined"
     ]
    }
   ],
   "source": [
    "find_code(1,prob_of_word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割文本一行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标点符号列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "stop_words = list('s~`!@#$%^&*()-？_+=[]{}〈〉nan;:\"\\'；！？～《》,<.>/?/g。：——「」 【】，|”（）、{ }()[]-“')\n",
    "f = open(\"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/停词库/chinese_symbols.txt\",'a+')\n",
    "for i in stop_words:\n",
    "    f.writelines(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查后一个词是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-84494fe86549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'特朗普当局似乎有点着急，这是一个好事情'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "input_text = split_paper('特朗普当局似乎有点着急，这是一个好事情')\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}