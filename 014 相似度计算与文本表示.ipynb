{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_path = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/停词库/stop_word_for_chinese.txt\"\n",
    "\n",
    "def del_element(strings,symbles):\n",
    "    srcrep = {i:'' for i in symbles }\n",
    "    rep = dict((re.escape(k), v) for k, v in srcrep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], strings)\n",
    "    \n",
    "def filter_stop_word(strings,stop_word=np.loadtxt(stop_word_path,dtype=str)):\n",
    "    return del_element(strings,stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    return open(path,'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/豆瓣电影数据集(2019.3)/豆瓣电影简介.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(corpus,n):\n",
    "    return np.array(list(jieba.cut(filter_stop_word(filter_stop_word(read_txt(corpus)[n]),'\\n '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ok: THUOCL_food.txt\nok: THUOCL_chengyu.txt\nok: THUOCL_caijing.txt\nok: THUOCL_law.txt\nok: THUOCL_medical.txt\nok: THUOCL_poem.txt\nok: THUOCL_animal.txt\nok: THUOCL_lishimingren.txt\nerror: THUOCL_it.txt Wrong number of columns at line 89\nok: THUOCL_diming.txt\nok: THUOCL_car.txt\n"
    }
   ],
   "source": [
    "def just_number(string, resymbol=\"\"):\n",
    "    sub_str = re.sub(u\"([^\\u0030-\\u0039])\", resymbol, string)\n",
    "    return sub_str\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def filter_dict(words,numbers):\n",
    "    word_dict ,expr_sum = dict(),0\n",
    "    for word,num in zip(words,numbers):\n",
    "        try:\n",
    "            number = float(just_number(num))\n",
    "            expr = {word:number}  \n",
    "        except Exception:\n",
    "            number = 0\n",
    "        finally:\n",
    "            expr_sum += number\n",
    "            word_dict.update(expr)\n",
    "    prob = {word:word_dict[word]/expr_sum for word in word_dict}\n",
    "    return prob\n",
    "\n",
    "def location_dict(dir_path):\n",
    "    init_dict = np.zeros((2,2))\n",
    "    for path in os.listdir(dir_path):\n",
    "        try:\n",
    "            file_path = \"{}{}\".format(dir_path,path)\n",
    "            file_of_one = np.loadtxt(file_path,delimiter='\\t',dtype=str)\n",
    "            print(\"ok:\",path)\n",
    "        except Exception as error:\n",
    "            file_of_one = np.array([line.replace(\"\\n\",\"\").split(\"\\t\") for line in open(file_path,'r').readlines()])\n",
    "            print(\"error:\",path,error)\n",
    "        finally :\n",
    "            init_dict = np.r_[init_dict,file_of_one]\n",
    "    words,numbers = init_dict[2:,0],init_dict[2:,1]\n",
    "    return filter_dict(words,numbers)\n",
    "\n",
    "\n",
    "\n",
    "dir_path = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/词库/chinese/\"\n",
    "words_dict = location_dict(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from /usr/local/lib/python3.7/site-packages/jieba/dict.txt ...\nLoading model from cache /var/folders/sl/q8x6_03132dfk7rktf00yh880000gn/T/jieba.cache\nLoading model cost 1.4183447360992432 seconds.\nPrefix dict has been built succesfully.\n"
    }
   ],
   "source": [
    "corpus = \"/Users/manmanzhang/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/NLP数据集合/豆瓣电影数据集(2019.3)/豆瓣电影简介.txt\"\n",
    "src1,src2 = split_word(corpus,1),split_word(corpus,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['里昂', '雷诺', '饰名', '孤', '职业杀手', '受', '雇佣', '天', '邻居家', '姑娘', '马蒂',\n        '达纳', '塔丽', '波特曼', '饰', '敲开', '房门', '求里', '暂避', '杀身', '祸', '邻居家',\n        '主警', '缉毒', '组', '眼线', '贪污', '包', '毒品', '遭恶警', '加里', '奥德曼', '饰',\n        '杀害', '全家', '惩罚', '马蒂达', '里昂', '留救', '幸免', '难留', '里昂', '里', '里昂',\n        '教', '女孩', '枪教', '里昂', '法文', '两', '关系', '日趋', '亲密', '相处', '融洽'],\n       dtype='<U4'),\n array(['年月日', '号称', '世界', '工业', '史迹', '豪华', '客轮', '泰坦尼克号', '处女航', '英国',\n        '南安普顿', '发驶', '美国纽约', '富家', '少女', '罗丝', '凯特', '•', '温丝', '莱特',\n        '母亲', '未婚夫', '卡坐', '头舱', '放荡', '羁', '少年', '画家', '杰', '克莱昂', '纳迪',\n        '卡', '普里', '奥', '码头', '场', '赌博', '中', '赢', '舱', '船票'], dtype='<U5'))"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "src1,src2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欧式距离 （相似度越大，距离越小）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_boolean(strings,words_dict):\n",
    "    words_vector = np.array([word for word in words_dict])\n",
    "    return np.array([words_vector==s for s in strings]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 ,s2 = count_boolean(src1,words_dict),count_boolean(src2,words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(s1,s2):\n",
    "    return np.sqrt(np.power(s1-s2,2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.23606797749979"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "euclidean(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 雅卡尔系数 （不需要相同编码向量，直接可以比较,相似度越大，值越大）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1,s2):\n",
    "    return np.intersect1d(s1,s2).shape[0]/np.union1d(s1,s2).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1.0, 0.0)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "jaccard(src1,src1),jaccard(src1,src2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 余弦相似度 （相似度越大，值越大）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(s1,s2):\n",
    "    return s1.dot(s2)/(np.linalg.norm(s1) * np.linalg.norm(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.0, 1.0000000000000002)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "cosine(s1,s2),cosine(s1,s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 曼哈顿距离 (相似度越大，值越小) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(s1, s2):\n",
    "    n = min(s1.size,s2.size)\n",
    "    return np.abs(s1-s2).dot(np.ones(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(5.0, 0.0)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "manhattan(s1,s2),manhattan(s1,s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切比雪夫距离 (相似度越高,值越大,寻找的是文本当中的词最大的频次差异)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev(s1, s2):\n",
    "    return np.max(np.abs(s1 - s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 0)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "chebyshev(s1,s2),chebyshev(s1,s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 闵可夫斯基距离,闵可夫斯基空间不同于牛顿力学的平坦空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski(x, y, p):\n",
    "    return np.sum(np.abs(x - y) ** p) ** (1 / p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1.7099759466766968, 0.0)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "minkowski(s1,s2,3),minkowski(s1,s1,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 汉明距离 (统计的是相同位置里相同的数比例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(x, y):\n",
    "    return sum(x!=y)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3.191462200321699e-05"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "hamming(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[  6, -10,  -6],\n        [  6,  -8,  -8],\n        [ -2,  -2,  -6],\n        [ -2,  -8,   0],\n        [  0,  -2,  -8],\n        [  0, -10,   0]]),\n array([ 3, -5, -3]),\n array([[ 3, -5, -3],\n        [ 3, -3, -5],\n        [-5,  3, -3],\n        [-5, -3,  3],\n        [-3,  3, -5],\n        [-3, -5,  3]]))"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = 3\n",
    "n = -5\n",
    "e = -3\n",
    "import itertools\n",
    "S1 = np.array([y,n,e])\n",
    "S = np.array(list(itertools.permutations(S1,3)))\n",
    "S1+S,S1,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "start = np.zeros_like(S)\n",
    "start"
   ]
  }
 ]
}